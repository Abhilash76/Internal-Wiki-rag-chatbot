{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ollama\n",
      "Version: 0.1.9\n",
      "Summary: The official Python client for Ollama.\n",
      "Home-page: https://ollama.ai\n",
      "Author: Ollama\n",
      "Author-email: hello@ollama.com\n",
      "License: MIT\n",
      "Location: c:\\users\\abhilash kandarpa\\anaconda3\\wikirag\\ragenv\\lib\\site-packages\n",
      "Requires: httpx\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL = \"llama2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "model = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's one:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\\n\\nHope that made you smile! Do you want to hear another?\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "chain.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | MediumOpen in appSign upSign inWriteSign upSign inMastodonIntroduction to Retrieval-Augmented Generation (RAG)Pankaj Pandey·Follow6 min read·Dec 16, 2023--ListenShareRAG systems aim to address the drawbacks of Large Language Models by incorporating factual information during response generation, mitigating issues such as knowledge cutoff and response hallucination.Retrieval Augmented Generation (RAG)The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have drawbacks due to their knowledge cutoff and other reasons, leading to confident but inaccurate responses. The RAG systems aim to address this issue by incorporating factual information during response generation to prevent hallucination and retrieve accurate responses.Introduction:RAG is an AI framework that improves the', metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have…', 'language': 'en'}),\n",
       " Document(page_content='to address this issue by incorporating factual information during response generation to prevent hallucination and retrieve accurate responses.Introduction:RAG is an AI framework that improves the accuracy and reliability of large language models (LLMs) by grounding them in external knowledge bases.LLMs can be inconsistent and prone to errors, lacking true understanding of word meaning.RAG addresses these issues by providing access to up-to-date facts and verifiable sources, increasing user trust.Purpose of RAG:Grounding LLMs on external knowledge for improved responses.Overcoming inconsistencies in LLM-generated answers.Challenges Addressed by RAG:Inconsistency in LLM responses.Lack of understanding of the meaning of words by LLMs.Reduction of opportunities for the model to leak sensitive data.Benefits of RAG:Accuracy and Fact-Checking:Ensures LLM responses are based on reliable sources, allowing users to verify claims.Reduced Bias and Hallucination:Limits LLM reliance on internal', metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have…', 'language': 'en'}),\n",
       " Document(page_content=\"data.Benefits of RAG:Accuracy and Fact-Checking:Ensures LLM responses are based on reliable sources, allowing users to verify claims.Reduced Bias and Hallucination:Limits LLM reliance on internal biases and prevents fabrication of information.Lower Cost and Maintenance:Reduces the need for continuous LLM retraining and updates, saving computational resources.How RAG Works:RAG consists of two distinct phases: retrieval and content generation. In the retrieval phase, algorithms search for and retrieve relevant information from external knowledge bases. This information is then used in the generative phase, where the LLM synthesizes an answer based on both the augmented prompt and its internal representation of training data.Phase 1: RetrievalRelevant information is retrieved from external sources based on the user's prompt or question.Sources vary depending on the context (open-domain internet vs. closed-domain enterprise data).Phase 2: Content GenerationThe retrieved information is\", metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have…', 'language': 'en'}),\n",
       " Document(page_content=\"sources based on the user's prompt or question.Sources vary depending on the context (open-domain internet vs. closed-domain enterprise data).Phase 2: Content GenerationThe retrieved information is appended to the user's prompt and fed to the LLM.The LLM generates a personalized answer based on the augmented prompt and its internal knowledge base.The answer can be delivered with links to its sources for transparency.Advantages and Applications of RAGRAG offers several advantages, including access to the latest, reliable facts, reduction in sensitive data leakage and decreased need for continuous model retraining. It finds applications in personalized responses, verifiable answers and lowering computational and financial costs in enterprise settings.Access to current and reliable information.Reduced opportunities for sensitive data leakage.Lower computational and financial costs in LLM-powered applications.Implementation and WorkflowsRAG operates in an “open book” manner, allowing LLMs\", metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have…', 'language': 'en'}),\n",
       " Document(page_content='opportunities for sensitive data leakage.Lower computational and financial costs in LLM-powered applications.Implementation and WorkflowsRAG operates in an “open book” manner, allowing LLMs to respond to questions by browsing through external content. It involves a retrieval phase, where relevant information is gathered and a generative phase, where the LLM synthesizes a response using both external and internal knowledge.Open-Book Approach:Contrasted with closed-book exams, where LLMs rely on memory.Model’s response involves browsing through external content.Workflow:Retrieval phase: Search and gather external information.Generative phase: Synthesize a personalized answer using internal and external knowledge.Teaching LLMs to Recognize LimitationsLLMs, when faced with ambiguous or complex queries, may provide inaccurate responses. RAG helps in training LLMs to recognize unanswerable questions and prompts them to either admit uncertainty or ask clarifying questions.Recognition of', metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have…', 'language': 'en'}),\n",
       " Document(page_content='queries, may provide inaccurate responses. RAG helps in training LLMs to recognize unanswerable questions and prompts them to either admit uncertainty or ask clarifying questions.Recognition of Limitations:LLMs prone to making things up in challenging scenarios.Training LLMs to explicitly recognize unanswerable questions.Challenges and Ongoing Innovations in Retrieval-Augmented GenerationWhile RAG is a powerful tool, there are still some challenges persist and ongoing innovations are necessary. Lots of Research is focused on improving both the retrieval and generation ends of the process to enhance the effectiveness of RAG.Challenges:Imperfections in RAG.Enriching prompts with relevant information using vectors.Innovations and Research:Focus on retrieval: Finding and fetching the most relevant information.Focus on generation: Structuring information for richer responses.Example Scenario: Customer Care ChatbotAlice, an employee, asks about taking vacation in half-day increments.The LLM', metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have…', 'language': 'en'}),\n",
       " Document(page_content='information.Focus on generation: Structuring information for richer responses.Example Scenario: Customer Care ChatbotAlice, an employee, asks about taking vacation in half-day increments.The LLM retrieves relevant data from Alice\\'s HR files and company policies.It generates a personalized answer confirming her vacation allowance and half-day eligibility.The answer is delivered with links to the HR files and policies for verification.Challenges and Future DirectionsTraining LLMs to recognize \"unknowns\" and avoid making things up.Improving retrieval algorithms for finding the most relevant information.Optimizing generation techniques for incorporating retrieved information effectively.Python Code Examples:1. Simple Retrieval with Elasticsearch:from elasticsearch import Elasticsearches = Elasticsearch()query = \"early dismissal school Wednesdays\"results = es.search(index=\"documents\", query={\"query\": {\"match\": {\"text\": query}}})# Process results and use them to augment the LLM prompt...2.', metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have…', 'language': 'en'}),\n",
       " Document(page_content='= \"early dismissal school Wednesdays\"results = es.search(index=\"documents\", query={\"query\": {\"match\": {\"text\": query}}})# Process results and use them to augment the LLM prompt...2. Combining Retrieval and Generation with Transformers:# Using a transformer-based model with RAG for information retrieval and generationfrom transformers import pipelinerag_qa_pipeline = pipeline(\"question-answering\", model=\"facebook/rag-token-base\", retriever=\"facebook/rag-token-base\")question = \"What is retrieval-augmented generation?\"answer = rag_qa_pipeline(question, truncation=True, return_prompt=True)print(answer)from transformers import AutoTokenizer, AutoModelForSequenceClassificationtokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")model = AutoModelForSequenceClassification.from_pretrained(\"rag/bert-base-uncased-rag\")prompt = \"Alice asks about taking vacation in half-day increments.\"retrieved_info = ...  # Retrieved information from external sourceencoded_prompt = tokenizer(prompt +', metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have…', 'language': 'en'}),\n",
       " Document(page_content='= \"Alice asks about taking vacation in half-day increments.\"retrieved_info = ...  # Retrieved information from external sourceencoded_prompt = tokenizer(prompt + retrieved_info, return_tensors=\"pt\")output = model(**encoded_prompt)# Process output and use it to generate the LLM response...# Sample Python code demonstrating the retrieval phase in RAGdef retrieval_phase(user_prompt, external_data):    # Algorithm to search and retrieve relevant information    retrieved_info = search_and_retrieve(user_prompt, external_data)    return retrieved_info# Sample Python code demonstrating the generative phase in RAGdef generative_phase(augmented_prompt, internal_data):    # Algorithm to synthesize a personalized answer using internal and external knowledge    answer = generate_response(augmented_prompt, internal_data)    return answer3. RAG Example in a question answering system:# Example of using RAG in a question answering systemfrom transformers import RagTokenizer, RagRetriever,', metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have…', 'language': 'en'}),\n",
       " Document(page_content='internal_data)    return answer3. RAG Example in a question answering system:# Example of using RAG in a question answering systemfrom transformers import RagTokenizer, RagRetriever, RagSequenceForGenerationtokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-base\")retriever = RagRetriever.from_pretrained(\"facebook/rag-token-base\")model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-base\")# Retrieve relevant informationinput_text = \"What is retrieval-augmented generation?\"retrieved_info = retriever.retrieve(input_text)# Generate response using RAGgenerated_response = model.generate(**retrieved_info)print(tokenizer.decode(generated_response[0], skip_special_tokens=True))4. Personalized and Verifiable Responses with RAGRetrieval-augmented generation (RAG) enables large language models (LLMs) to provide personalized responses without constant retraining. It reduces the need for manual scripting in chatbots, allowing the model to adapt to new information', metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have…', 'language': 'en'}),\n",
       " Document(page_content='large language models (LLMs) to provide personalized responses without constant retraining. It reduces the need for manual scripting in chatbots, allowing the model to adapt to new information dynamically.# Using RAG to generate personalized responses in a chatbotfrom transformers import RagRetriever, RagSequenceForGenerationretriever = RagRetriever.from_pretrained(\"facebook/rag-token-base\")model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-base\")# User\\'s queryuser_query = \"Can I take vacation in half-day increments?\"# Retrieve relevant informationretrieved_info = retriever.retrieve(user_query)# Generate personalized response using RAGgenerated_response = model.generate(**retrieved_info)print(tokenizer.decode(generated_response[0], skip_special_tokens=True))5. Teaching the Model to Recognize UnknownsLLMs need explicit training to recognize and admit when they cannot answer certain questions. RAG addresses the challenge of ambiguous queries, complex questions and', metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have…', 'language': 'en'}),\n",
       " Document(page_content='the Model to Recognize UnknownsLLMs need explicit training to recognize and admit when they cannot answer certain questions. RAG addresses the challenge of ambiguous queries, complex questions and situations where the model lacks information.# Training an LLM to recognize unknown questions using RAGfrom transformers import RagRetriever, RagSequenceForGenerationretriever = RagRetriever.from_pretrained(\"facebook/rag-token-base\")model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-base\")# User\\'s ambiguous queryambiguous_query = \"How much vacation time do I have?\"# Retrieve relevant informationretrieved_info = retriever.retrieve(ambiguous_query)# Train the model to recognize unknowns# ...# Generate response using RAG, considering the unknown recognitiongenerated_response = model.generate(**retrieved_info)print(tokenizer.decode(generated_response[0], skip_special_tokens=True))Note: These are simplified examples to understand the basic concepts, the actual implementation may', metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have…', 'language': 'en'}),\n",
       " Document(page_content='skip_special_tokens=True))Note: These are simplified examples to understand the basic concepts, the actual implementation may require more complex code and libraries depending on the specific use case and desired functionalities.Conclusion:RAG is a promising approach for improving LLM accuracy and reliability, offering benefits like factual grounding, reduced bias and lower maintenance costs. While challenges remain in areas like unknown recognition and retrieval optimization, ongoing research is pushing the boundaries of RAG capabilities and paving the way for more trustworthy and informative LLM applications.Supporting Information:Vector databases play a crucial role in RAG by efficiently indexing, storing and retrieving information.Lots of Research emphasizes the imperfections of RAG and the need for ongoing improvements in its implementation.Remember: This is a preliminary analysis. Further research and validation are needed to ensure the accuracy and completeness of the', metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have…', 'language': 'en'}),\n",
       " Document(page_content='of RAG and the need for ongoing improvements in its implementation.Remember: This is a preliminary analysis. Further research and validation are needed to ensure the accuracy and completeness of the information presented.SourcesData-Science-Pipeline-DetectorRetrieval-augmented-generation-RAGLarge Language ModelsRagMachine LearningData ScienceAI----FollowWritten by Pankaj Pandey225 FollowersExpert in software technologies with proficiency in multiple languages, experienced in Generative AI, NLP, Bigdata, and application development.FollowHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams', metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have…', 'language': 'en'})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d\")\n",
    "document = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "document_chunks = text_splitter.split_documents(document)\n",
    "\n",
    "document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vector_store = DocArrayInMemorySearch.from_documents(\n",
    "    document_chunks,\n",
    "    embedding = embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch at 0x24e58b2bf90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based on the following context.\n",
    "              If you can't answer the question, reply \"I'm a little fuzzy on the details.\n",
    "              Bur feel free to refer the blog.\"\n",
    "\n",
    "           Context: {context}\n",
    "           Question: {question}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    |prompt\n",
    "    |model\n",
    "    |parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = {\n",
    "    \"What is the benefit of Retrieval Augmented Generation?\" \n",
    "    \"What are the challenges in RAG?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    context = vector_store.similarity_search(question, k=1)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print(f\"Context:\", context,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Pennylane?\n",
      "Answer: I'm a little fuzzy on the details. The context provided does not mention Pennylane. It appears to be discussing Retrieval-Augmented Generation (RAG) and its applications in large language models (LLMs). If you'd like, I can refer you to the blog post for more information on RAG!\n",
      "Context: [Document(page_content='the Model to Recognize UnknownsLLMs need explicit training to recognize and admit when they cannot answer certain questions. RAG addresses the challenge of ambiguous queries, complex questions and situations where the model lacks information.# Training an LLM to recognize unknown questions using RAGfrom transformers import RagRetriever, RagSequenceForGenerationretriever = RagRetriever.from_pretrained(\"facebook/rag-token-base\")model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-base\")# User\\'s ambiguous queryambiguous_query = \"How much vacation time do I have?\"# Retrieve relevant informationretrieved_info = retriever.retrieve(ambiguous_query)# Train the model to recognize unknowns# ...# Generate response using RAG, considering the unknown recognitiongenerated_response = model.generate(**retrieved_info)print(tokenizer.decode(generated_response[0], skip_special_tokens=True))Note: These are simplified examples to understand the basic concepts, the actual implementation may', metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have…', 'language': 'en'}), Document(page_content='= \"Alice asks about taking vacation in half-day increments.\"retrieved_info = ...  # Retrieved information from external sourceencoded_prompt = tokenizer(prompt + retrieved_info, return_tensors=\"pt\")output = model(**encoded_prompt)# Process output and use it to generate the LLM response...# Sample Python code demonstrating the retrieval phase in RAGdef retrieval_phase(user_prompt, external_data):    # Algorithm to search and retrieve relevant information    retrieved_info = search_and_retrieve(user_prompt, external_data)    return retrieved_info# Sample Python code demonstrating the generative phase in RAGdef generative_phase(augmented_prompt, internal_data):    # Algorithm to synthesize a personalized answer using internal and external knowledge    answer = generate_response(augmented_prompt, internal_data)    return answer3. RAG Example in a question answering system:# Example of using RAG in a question answering systemfrom transformers import RagTokenizer, RagRetriever,', metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have…', 'language': 'en'}), Document(page_content='opportunities for sensitive data leakage.Lower computational and financial costs in LLM-powered applications.Implementation and WorkflowsRAG operates in an “open book” manner, allowing LLMs to respond to questions by browsing through external content. It involves a retrieval phase, where relevant information is gathered and a generative phase, where the LLM synthesizes a response using both external and internal knowledge.Open-Book Approach:Contrasted with closed-book exams, where LLMs rely on memory.Model’s response involves browsing through external content.Workflow:Retrieval phase: Search and gather external information.Generative phase: Synthesize a personalized answer using internal and external knowledge.Teaching LLMs to Recognize LimitationsLLMs, when faced with ambiguous or complex queries, may provide inaccurate responses. RAG helps in training LLMs to recognize unanswerable questions and prompts them to either admit uncertainty or ask clarifying questions.Recognition of', metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have…', 'language': 'en'})] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Pennylane?\"\n",
    "\n",
    "context = vector_store.similarity_search(question, k=1)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "print(f\"Context:\", context,\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
