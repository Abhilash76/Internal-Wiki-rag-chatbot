{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/project/, https://kandarpa%40data-cybernetics.com:****@pkgs.dev.azure.com/data-cybernetics/_packaging/data-cybernetics/pypi/simple/\n",
      "Requirement already satisfied: ollama in c:\\users\\abhilash kandarpa\\anaconda3\\wikirag\\ragenv\\lib\\site-packages (0.1.9)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\abhilash kandarpa\\anaconda3\\wikirag\\ragenv\\lib\\site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\abhilash kandarpa\\anaconda3\\wikirag\\ragenv\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\abhilash kandarpa\\anaconda3\\wikirag\\ragenv\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\abhilash kandarpa\\anaconda3\\wikirag\\ragenv\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\abhilash kandarpa\\anaconda3\\wikirag\\ragenv\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\abhilash kandarpa\\anaconda3\\wikirag\\ragenv\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\abhilash kandarpa\\anaconda3\\wikirag\\ragenv\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL = \"gemma2:2b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "model = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't scientists trust atoms? \\n\\nBecause they make up everything! üòÑ  \\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "chain.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content='Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | MediumOpen in appSign upSign inWriteSign upSign inMastodonIntroduction to Retrieval-Augmented Generation (RAG)Pankaj Pandey¬∑Follow6 min read¬∑Dec 16, 2023--ListenShareRAG systems aim to address the drawbacks of Large Language Models by incorporating factual information during response generation, mitigating issues such as knowledge cutoff and response hallucination.Retrieval Augmented Generation (RAG)The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have drawbacks due to their knowledge cutoff and other reasons, leading to confident but inaccurate responses. The RAG systems aim to address this issue by incorporating factual information during response generation to prevent hallucination and retrieve accurate responses.Introduction:RAG is an AI framework that improves the'),\n",
       " Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content='to address this issue by incorporating factual information during response generation to prevent hallucination and retrieve accurate responses.Introduction:RAG is an AI framework that improves the accuracy and reliability of large language models (LLMs) by grounding them in external knowledge bases.LLMs can be inconsistent and prone to errors, lacking true understanding of word meaning.RAG addresses these issues by providing access to up-to-date facts and verifiable sources, increasing user trust.Purpose of RAG:Grounding LLMs on external knowledge for improved responses.Overcoming inconsistencies in LLM-generated answers.Challenges Addressed by RAG:Inconsistency in LLM responses.Lack of understanding of the meaning of words by LLMs.Reduction of opportunities for the model to leak sensitive data.Benefits of RAG:Accuracy and Fact-Checking:Ensures LLM responses are based on reliable sources, allowing users to verify claims.Reduced Bias and Hallucination:Limits LLM reliance on internal'),\n",
       " Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content=\"data.Benefits of RAG:Accuracy and Fact-Checking:Ensures LLM responses are based on reliable sources, allowing users to verify claims.Reduced Bias and Hallucination:Limits LLM reliance on internal biases and prevents fabrication of information.Lower Cost and Maintenance:Reduces the need for continuous LLM retraining and updates, saving computational resources.How RAG Works:RAG consists of two distinct phases: retrieval and content generation. In the retrieval phase, algorithms search for and retrieve relevant information from external knowledge bases. This information is then used in the generative phase, where the LLM synthesizes an answer based on both the augmented prompt and its internal representation of training data.Phase 1: RetrievalRelevant information is retrieved from external sources based on the user's prompt or question.Sources vary depending on the context (open-domain internet vs. closed-domain enterprise data).Phase 2: Content GenerationThe retrieved information is\"),\n",
       " Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content=\"sources based on the user's prompt or question.Sources vary depending on the context (open-domain internet vs. closed-domain enterprise data).Phase 2: Content GenerationThe retrieved information is appended to the user's prompt and fed to the LLM.The LLM generates a personalized answer based on the augmented prompt and its internal knowledge base.The answer can be delivered with links to its sources for transparency.Advantages and Applications of RAGRAG offers several advantages, including access to the latest, reliable facts, reduction in sensitive data leakage and decreased need for continuous model retraining. It finds applications in personalized responses, verifiable answers and lowering computational and financial costs in enterprise settings.Access to current and reliable information.Reduced opportunities for sensitive data leakage.Lower computational and financial costs in LLM-powered applications.Implementation and WorkflowsRAG operates in an ‚Äúopen book‚Äù manner, allowing LLMs\"),\n",
       " Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content='opportunities for sensitive data leakage.Lower computational and financial costs in LLM-powered applications.Implementation and WorkflowsRAG operates in an ‚Äúopen book‚Äù manner, allowing LLMs to respond to questions by browsing through external content. It involves a retrieval phase, where relevant information is gathered and a generative phase, where the LLM synthesizes a response using both external and internal knowledge.Open-Book Approach:Contrasted with closed-book exams, where LLMs rely on memory.Model‚Äôs response involves browsing through external content.Workflow:Retrieval phase: Search and gather external information.Generative phase: Synthesize a personalized answer using internal and external knowledge.Teaching LLMs to Recognize LimitationsLLMs, when faced with ambiguous or complex queries, may provide inaccurate responses. RAG helps in training LLMs to recognize unanswerable questions and prompts them to either admit uncertainty or ask clarifying questions.Recognition of'),\n",
       " Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content='queries, may provide inaccurate responses. RAG helps in training LLMs to recognize unanswerable questions and prompts them to either admit uncertainty or ask clarifying questions.Recognition of Limitations:LLMs prone to making things up in challenging scenarios.Training LLMs to explicitly recognize unanswerable questions.Challenges and Ongoing Innovations in Retrieval-Augmented GenerationWhile RAG is a powerful tool, there are still some challenges persist and ongoing innovations are necessary. Lots of Research is focused on improving both the retrieval and generation ends of the process to enhance the effectiveness of RAG.Challenges:Imperfections in RAG.Enriching prompts with relevant information using vectors.Innovations and Research:Focus on retrieval: Finding and fetching the most relevant information.Focus on generation: Structuring information for richer responses.Example Scenario: Customer Care ChatbotAlice, an employee, asks about taking vacation in half-day increments.The LLM'),\n",
       " Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content='information.Focus on generation: Structuring information for richer responses.Example Scenario: Customer Care ChatbotAlice, an employee, asks about taking vacation in half-day increments.The LLM retrieves relevant data from Alice\\'s HR files and company policies.It generates a personalized answer confirming her vacation allowance and half-day eligibility.The answer is delivered with links to the HR files and policies for verification.Challenges and Future DirectionsTraining LLMs to recognize \"unknowns\" and avoid making things up.Improving retrieval algorithms for finding the most relevant information.Optimizing generation techniques for incorporating retrieved information effectively.Python Code Examples:1. Simple Retrieval with Elasticsearch:from elasticsearch import Elasticsearches = Elasticsearch()query = \"early dismissal school Wednesdays\"results = es.search(index=\"documents\", query={\"query\": {\"match\": {\"text\": query}}})# Process results and use them to augment the LLM prompt...2.'),\n",
       " Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content='= \"early dismissal school Wednesdays\"results = es.search(index=\"documents\", query={\"query\": {\"match\": {\"text\": query}}})# Process results and use them to augment the LLM prompt...2. Combining Retrieval and Generation with Transformers:# Using a transformer-based model with RAG for information retrieval and generationfrom transformers import pipelinerag_qa_pipeline = pipeline(\"question-answering\", model=\"facebook/rag-token-base\", retriever=\"facebook/rag-token-base\")question = \"What is retrieval-augmented generation?\"answer = rag_qa_pipeline(question, truncation=True, return_prompt=True)print(answer)from transformers import AutoTokenizer, AutoModelForSequenceClassificationtokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")model = AutoModelForSequenceClassification.from_pretrained(\"rag/bert-base-uncased-rag\")prompt = \"Alice asks about taking vacation in half-day increments.\"retrieved_info = ...  # Retrieved information from external sourceencoded_prompt = tokenizer(prompt +'),\n",
       " Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content='= \"Alice asks about taking vacation in half-day increments.\"retrieved_info = ...  # Retrieved information from external sourceencoded_prompt = tokenizer(prompt + retrieved_info, return_tensors=\"pt\")output = model(**encoded_prompt)# Process output and use it to generate the LLM response...# Sample Python code demonstrating the retrieval phase in RAGdef retrieval_phase(user_prompt, external_data):    # Algorithm to search and retrieve relevant information    retrieved_info = search_and_retrieve(user_prompt, external_data)    return retrieved_info# Sample Python code demonstrating the generative phase in RAGdef generative_phase(augmented_prompt, internal_data):    # Algorithm to synthesize a personalized answer using internal and external knowledge    answer = generate_response(augmented_prompt, internal_data)    return answer3. RAG Example in a question answering system:# Example of using RAG in a question answering systemfrom transformers import RagTokenizer, RagRetriever,'),\n",
       " Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content='internal_data)    return answer3. RAG Example in a question answering system:# Example of using RAG in a question answering systemfrom transformers import RagTokenizer, RagRetriever, RagSequenceForGenerationtokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-base\")retriever = RagRetriever.from_pretrained(\"facebook/rag-token-base\")model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-base\")# Retrieve relevant informationinput_text = \"What is retrieval-augmented generation?\"retrieved_info = retriever.retrieve(input_text)# Generate response using RAGgenerated_response = model.generate(**retrieved_info)print(tokenizer.decode(generated_response[0], skip_special_tokens=True))4. Personalized and Verifiable Responses with RAGRetrieval-augmented generation (RAG) enables large language models (LLMs) to provide personalized responses without constant retraining. It reduces the need for manual scripting in chatbots, allowing the model to adapt to new information'),\n",
       " Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content='large language models (LLMs) to provide personalized responses without constant retraining. It reduces the need for manual scripting in chatbots, allowing the model to adapt to new information dynamically.# Using RAG to generate personalized responses in a chatbotfrom transformers import RagRetriever, RagSequenceForGenerationretriever = RagRetriever.from_pretrained(\"facebook/rag-token-base\")model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-base\")# User\\'s queryuser_query = \"Can I take vacation in half-day increments?\"# Retrieve relevant informationretrieved_info = retriever.retrieve(user_query)# Generate personalized response using RAGgenerated_response = model.generate(**retrieved_info)print(tokenizer.decode(generated_response[0], skip_special_tokens=True))5. Teaching the Model to Recognize UnknownsLLMs need explicit training to recognize and admit when they cannot answer certain questions. RAG addresses the challenge of ambiguous queries, complex questions and'),\n",
       " Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content='the Model to Recognize UnknownsLLMs need explicit training to recognize and admit when they cannot answer certain questions. RAG addresses the challenge of ambiguous queries, complex questions and situations where the model lacks information.# Training an LLM to recognize unknown questions using RAGfrom transformers import RagRetriever, RagSequenceForGenerationretriever = RagRetriever.from_pretrained(\"facebook/rag-token-base\")model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-base\")# User\\'s ambiguous queryambiguous_query = \"How much vacation time do I have?\"# Retrieve relevant informationretrieved_info = retriever.retrieve(ambiguous_query)# Train the model to recognize unknowns# ...# Generate response using RAG, considering the unknown recognitiongenerated_response = model.generate(**retrieved_info)print(tokenizer.decode(generated_response[0], skip_special_tokens=True))Note: These are simplified examples to understand the basic concepts, the actual implementation may'),\n",
       " Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content='skip_special_tokens=True))Note: These are simplified examples to understand the basic concepts, the actual implementation may require more complex code and libraries depending on the specific use case and desired functionalities.Conclusion:RAG is a promising approach for improving LLM accuracy and reliability, offering benefits like factual grounding, reduced bias and lower maintenance costs. While challenges remain in areas like unknown recognition and retrieval optimization, ongoing research is pushing the boundaries of RAG capabilities and paving the way for more trustworthy and informative LLM applications.Supporting Information:Vector databases play a crucial role in RAG by efficiently indexing, storing and retrieving information.Lots of Research emphasizes the imperfections of RAG and the need for ongoing improvements in its implementation.Remember: This is a preliminary analysis. Further research and validation are needed to ensure the accuracy and completeness of the'),\n",
       " Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content='of RAG and the need for ongoing improvements in its implementation.Remember: This is a preliminary analysis. Further research and validation are needed to ensure the accuracy and completeness of the information presented.SourcesData-Science-Pipeline-DetectorRetrieval-augmented-generation-RAGLarge Language ModelsRagMachine LearningData ScienceAI----FollowWritten by Pankaj Pandey287 FollowersExpert in software technologies with proficiency in multiple languages, experienced in Generative AI, NLP, Bigdata, and application development.FollowHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d\")\n",
    "document = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "document_chunks = text_splitter.split_documents(document)\n",
    "\n",
    "document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\abhilash kandarpa\\anaconda3\\wikirag\\ragenv\\Lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vector_store = DocArrayInMemorySearch.from_documents(\n",
    "    document_chunks,\n",
    "    embedding = embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch at 0x1fdf4b6cd90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based on the following context.\n",
    "              If you can't answer the question, reply \"I'm a little fuzzy on the details.\n",
    "              Bur feel free to refer the blog.\"\n",
    "\n",
    "           Context: {context}\n",
    "           Question: {question}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    |prompt\n",
    "    |model\n",
    "    |parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = {\n",
    "    \"What is the benefit of Retrieval Augmented Generation?\" \n",
    "    \"What are the challenges in RAG?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the benefit of Retrieval Augmented Generation?What are the challenges in RAG?\n",
      "Answer: Retrieval Augmented Generation (RAG) offers several benefits, including:\n",
      "\n",
      "* **Factual grounding:**  It helps LLMs provide accurate information by grounding their responses in real data from a variety of sources. This makes for more reliable and truthful outputs. \n",
      "* **Reduced bias:** RAG can help mitigate biases present in the LLM's training data, leading to less skewed and fairer results.\n",
      "* **Lower maintenance costs:** The integration of pre-existing datasets allows for easier updates and maintenance compared to LLMs trained solely on massive text datasets.\n",
      "\n",
      "\n",
      "Here are some challenges associated with RAG:\n",
      "\n",
      "* **Unanswerable question recognition:**  LLMs can sometimes struggle to recognize questions they don't know the answer to, leading to false claims or inaccurate responses when prompted with unknown information.\n",
      "* **Retrieval optimization:** Finding the most relevant information from vast amounts of data requires advanced algorithms that are still being developed and refined for RAG implementations. \n",
      "\n",
      "\n",
      "RAG presents a promising path towards more robust and accurate LLMs, but overcoming these challenges remains crucial for wider adoption and improved user experiences. \n",
      "\n",
      "Context: [Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content='of RAG and the need for ongoing improvements in its implementation.Remember: This is a preliminary analysis. Further research and validation are needed to ensure the accuracy and completeness of the information presented.SourcesData-Science-Pipeline-DetectorRetrieval-augmented-generation-RAGLarge Language ModelsRagMachine LearningData ScienceAI----FollowWritten by Pankaj Pandey287 FollowersExpert in software technologies with proficiency in multiple languages, experienced in Generative AI, NLP, Bigdata, and application development.FollowHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams'), Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content='information.Focus on generation: Structuring information for richer responses.Example Scenario: Customer Care ChatbotAlice, an employee, asks about taking vacation in half-day increments.The LLM retrieves relevant data from Alice\\'s HR files and company policies.It generates a personalized answer confirming her vacation allowance and half-day eligibility.The answer is delivered with links to the HR files and policies for verification.Challenges and Future DirectionsTraining LLMs to recognize \"unknowns\" and avoid making things up.Improving retrieval algorithms for finding the most relevant information.Optimizing generation techniques for incorporating retrieved information effectively.Python Code Examples:1. Simple Retrieval with Elasticsearch:from elasticsearch import Elasticsearches = Elasticsearch()query = \"early dismissal school Wednesdays\"results = es.search(index=\"documents\", query={\"query\": {\"match\": {\"text\": query}}})# Process results and use them to augment the LLM prompt...2.'), Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content='queries, may provide inaccurate responses. RAG helps in training LLMs to recognize unanswerable questions and prompts them to either admit uncertainty or ask clarifying questions.Recognition of Limitations:LLMs prone to making things up in challenging scenarios.Training LLMs to explicitly recognize unanswerable questions.Challenges and Ongoing Innovations in Retrieval-Augmented GenerationWhile RAG is a powerful tool, there are still some challenges persist and ongoing innovations are necessary. Lots of Research is focused on improving both the retrieval and generation ends of the process to enhance the effectiveness of RAG.Challenges:Imperfections in RAG.Enriching prompts with relevant information using vectors.Innovations and Research:Focus on retrieval: Finding and fetching the most relevant information.Focus on generation: Structuring information for richer responses.Example Scenario: Customer Care ChatbotAlice, an employee, asks about taking vacation in half-day increments.The LLM')] \n",
      "\n",
      "total time taken to generate answers:66.06469058990479\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for question in questions:\n",
    "    context = vector_store.similarity_search(question, k=3)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print(f\"Context:\", context,\"\\n\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"total time taken to generate answers:{end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Pennylane?\n",
      "Answer: I'm a little fuzzy on the details. \n",
      "\n",
      "Context: [Document(metadata={'source': 'https://medium.com/@pankaj_pandey/introduction-to-retrieval-augmented-generation-rag-9209bf8a076d', 'title': 'Introduction to Retrieval-Augmented Generation (RAG) | by Pankaj Pandey | Medium', 'description': 'The world is advancing rapidly, introducing new technologies and stacks in AI and other areas every day. Large Language Models (LLMs) are a significant innovation in this space. However, LLMs have‚Ä¶', 'language': 'en'}, page_content='the Model to Recognize UnknownsLLMs need explicit training to recognize and admit when they cannot answer certain questions. RAG addresses the challenge of ambiguous queries, complex questions and situations where the model lacks information.# Training an LLM to recognize unknown questions using RAGfrom transformers import RagRetriever, RagSequenceForGenerationretriever = RagRetriever.from_pretrained(\"facebook/rag-token-base\")model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-base\")# User\\'s ambiguous queryambiguous_query = \"How much vacation time do I have?\"# Retrieve relevant informationretrieved_info = retriever.retrieve(ambiguous_query)# Train the model to recognize unknowns# ...# Generate response using RAG, considering the unknown recognitiongenerated_response = model.generate(**retrieved_info)print(tokenizer.decode(generated_response[0], skip_special_tokens=True))Note: These are simplified examples to understand the basic concepts, the actual implementation may')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Pennylane?\"\n",
    "\n",
    "context = vector_store.similarity_search(question, k=1)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "print(f\"Context:\", context,\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
